\vspace{-2mm}

We have shown how to combine an old idea --- of interpretable, generative, layered models of images --- 
with modern techniques of deep learning, in order to tackle the challenging problem of intepreting images in the presence of occlusion in an entirely unsupervised fashion. We see this is as a crucial stepping stone to future work on deeper scene understanding, going beyond simple 
feedforward supervised prediction problems.
In the future, we would like to apply our approach to real images, and possibly video.
This will require extending our methods to use convolutional networks, and may
also require some weak supervision or curriculum learning to simplify the learning task.


\eat{
Probabilistic graphical models with generative semantics were popular in vision not too many years ago 
but in recent years have largely fallen out of favor. These more traditional generative models typically are burdened by slow inference and can be surprisingly unwieldy since good accuracy often relied on maintaining code for a large family of different handcrafted visual features which had to be tuned appropriately.  However our work suggests that we may not want to throw the baby out with the bathwater just yet.  
Probabilistic generative models often have interpretable  semantics  and allow us to learn with supervision, weak/semi-supervision and no supervision all within a unified framework.  Our models are examples of this type of interpretable generative model that \emph{can} coexist with rich feature hierarchies that are automatically tuned by end-to-end backpropagation. They are also fast at test time, requiring just a single forward pass through a neural network.  Finally they can be trained with no labeled data. Thus we believe that this combining of the best of both worlds will be a fruitful area of future work in both representation learning
and computer vision.
}