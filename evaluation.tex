
In all of our experiments we use the same training settings used in~\cite{Kingma2014}; that is,  
we use Adagrad for optimization with minibatches of 100  with a learning rate of 0.01
and a weight decay corresponding to a prior of $\mathcal{N}(0,1)$.
We initialize weights in our network using the heuristic of ~\cite{glorot2010understanding}.
However for the pose recognition modules in the ST-VAE model, we have found it useful to
specifically initialize biases so that poses are initially close to the identity transformation (see~\cite{jaderberg2015spatial}).

For all three models (VAE, ST-VAE, CST-VAE),
we experiment with between 10 and 50  dimensions for the latent variables $z$.
We parameterize content encoders and decoders
by using a two layer fully connected MLP with 256 dimensional
hidden layers and ReLU nonlinearities.
For pose decoders and encoders we also use two layer fully connected MLPs, but 
using 32 dimensional hidden layers and Tanh nonlinearities.\footnote{
%
We have found this choice of Tanh vs. ReLU makes a significant
difference in practice.
\kevin{Why?}
}
Finally for spatial transformer modules, we always resample onto a grid that is the same size as the original
image.


\Jon{For binary images, it is common to have a sigmoid at the end of the VAE decoder.
However we have found that it is often more numerically stable in these situations to first apple the spatial transformation, then
to apply the sigmoid after the transformation.
}

\subsection{Evaluating the Spatially transformed Variational Autoencoder alone}

We first evaluate our ST-VAE (single layer) model alone on the MNIST dataset~\citep{lecun1998gradient}
and a derived dataset, \emph{TranslatedMNIST}, in which we randomly translated each  $28\times 28$ MNIST example
within a $36\times 36$ black image.  In both cases, we binarize the images by thresholding at value 128.
Figure~\ref{fig:vae_stvae_learning_curves} plots train and test curves over 250000 gradient steps

Ordinary and Translated MNIST comparing
\begin{itemize}
\item ordinary vae
\item stvae model
\end{itemize}


\begin{itemize}
\item show pretty images
\item show that we often converge in first few iterations in pose and start to learn style after that
\end{itemize}



\Jon{Accuracy of classification of latent layer?}



\begin{figure}[t]
\begin{center}
\subfigure[]{
\raisebox{3mm}{
\includegraphics[width=0.45\linewidth]{figs/vae_stvae_samples.pdf}
\label{fig:vae_stvae_samples}
}
}
\qquad
\subfigure[]{
\includegraphics[width=0.45\linewidth]{figs/stvae_averageddigits.pdf}
\label{fig:stvae_averageddigits}
}
\end{center}
 \caption{
 \subref{fig:vae_stvae_samples} Comparison of samples from the VAE and ST-VAE generative models.  
 For the ST-VAE model, we show both the sample in its canonical pose and the final generated image.
  \subref{fig:stvae_averageddigits}  Averaged images from each MNIST class as learning progresses ---
  we typically see pose variables converge very quickly.
 }
\end{figure}

\begin{figure}[t]
\begin{center}
\subfigure[]{
\includegraphics[width=0.30\linewidth]{figs/vae_stvae_learning_curves.pdf}
\label{fig:vae_stvae_learning_curves}
}
\subfigure[]{
\raisebox{12mm}{
\footnotesize
\begin{tabular}{lcc} 
\hline 
%\multicolumn{1}{r|}{Proposals} & \multicolumn{2}{c}{GT} & \multicolumn{2}{c}{multibox} \\
%\multicolumn{1}{r|}{Descriptions} & GEN & GT & GEN & GT \\
 & Train Accuracy & Test Accuracy \\
\hline 
\multicolumn{3}{l}{On translated (36x36) MNIST} \\
\hline
VAE + supervised & 0.771 & 0.146 \\
ST-VAE + supervised & 0.972 & 0.964  \\
directly supervised & 0.884 & 0.783 \\
Directly supervised with STN & 0.993 & 0.969  \\
\hline 
\multicolumn{3}{l}{On original (28x28) MNIST} \\
\hline 
Directly supervised & 0.999 & 0.96   \\
\hline
\end{tabular}%
}
\label{fig:vae_stvae_classification}
}
\end{center}

 \caption{
 \subref{fig:vae_stvae_learning_curves} Train and test (per-example) lower bounds on log-likelihood 
for the vanilla VAE and ST-VAE models on the Translated MNIST data;
 \subref{fig:vae_stvae_classification} Classification accuracy obtained by supervised training using latent encodings
 from VAE and ST-VAE models.  More details in text.
 }
\end{figure}









\subsection{Evaluating the CST Variational Autoencoder}

Dataset
\begin{itemize}
\item MNIST superimposed
\item Can we also get this to work with silhouettes of different grayscale levels?
\item CIFAR???
\item Textures
\end{itemize}


Explain encoder/decoder architectures for pose and style

Show reconstruction results on MNIST and samples from a single STAEVB module within the network



\begin{figure}[t]
\begin{center}
\subfigure[]{
\includegraphics[width=0.4\linewidth]{figs/cstvae_reconstructions.pdf}
\label{fig:cstvae_reconstructions}
}\qquad\qquad
\subfigure[]{
\raisebox{5mm}{
\includegraphics[width=0.3\linewidth]{figs/cstvae_classification.pdf}
\label{fig:cstvae_classification}
}
}
\end{center}
 \caption{
 \subref{fig:cstvae_reconstructions}  Images from the Superimposed MNIST dataset with visualizations of
 intermediate variables in the neural network corresponding to first and second layers and the final reconstruction;
 \subref{fig:cstvae_classification}
 Classification accuracy obtained by supervised training using latent encodings
 from VAE and CST-VAE models on the Supervised MNIST dataset.  More details in text.
 }
\end{figure}




\subsection{Partially observed data}




\subsection{Supervised training}
compare accuracy on something



\subsection{With Textures???}












